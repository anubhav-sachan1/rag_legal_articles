'Firm	URL
Linklaters	https://www.linklaters.com/en/knowledge
Cooley	https://www.cooley.com/news/
Skadden	https://www.skadden.com/insights
Sidley	https://www.sidley.com/en/us/insights
Simpson Thacher	https://www.stblaw.com/about-us/news
Latham & Watkins	https://www.lw.com/en/insights-listing#sort=%40newsandinsightsdate%20descending
Goodwin	https://www.goodwinlaw.com/en/insights
Debevoise	https://www.debevoise.com/insights/search
Advant	https://www.advant-beiten.com/en/news/blog
CMS	https://cms.law/en/int/publication
Hengeler Mueller	https://hengeler-news.com/en/latest-articles
Gleiss Lutz	https://www.gleisslutz.com/en/news-events/know-how
Noerr	https://www.noerr.com/en/insights?i-fc-type=News

1. Finish Linklaters
2. Clean up Skadden to resemble others -> IMP
4. Unify all crawlers -> IMP
7. Add command line argument or config file to specify directories which have to be embedded/indexed -> IMP


3. Create a file for handling dependencies and creating a new Conda environment

5. Create a mutlithreading script to run all crawlers
6. Add multithreading in the embedding script


8. Maybe add headless mode for scraping?

9. Create README
10. Create ZIPs and submit



src
|__ scrapers
|__ chunker.py
|__ embed_chunks.py
|__ create_index.py
|__ launch_server.py
|__ user_query.py
run_build_index.sh <skip scraper>
  |_ scraper_main.py
  |_ chunker.py
  |_ embed_chunks.py
  |_ create_index.py
run_start_server.sh
run_user_query.sh
README
dependencies.yaml



